# Setup commands for lazy people who dont want to read the full pdf documentation.
# Run these commands as root to build a simple head + 2 compute node setup 
#    with nvidia gpu driver on the head node.
# The only user requirement is entering the MAC address of the 2 compute nodes.
# Instructions assume a clean CentOS8.3 installation, but are not unique to 
#    the africa cluster(s)/equipment.
# Note that some steps are for rebooting, so this script cannot be executed as-is.


dnf -y update

 ### Use the following 3 lines if installing after the transition to centOS stream releases   ###
 # (if stream is used, then the previous update command would fail)
#dnf install centos-release-stream
#dnf --disablerepo '*' --enablerepo=extras swap centos-linux-repos centos-stream-repos
#dnf distro-sync

dnf -y --enablerepo=extras install epel-release
dnf -y --enablerepo=baseos install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
reboot

dnf -y install http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm
dnf -y install wget
wget -P /etc/yum.repos.d https://xcat.org/files/xcat/repos/yum/latest/xcat-core/xcat-core.repo
wget -P /etc/yum.repos.d http://xcat.org/files/xcat/repos/yum/devel/xcat-dep/rh8/x86_64/xcat-dep.repo
dnf -y install dnf-plugins-core
dnf config-manager --set-enabled powertools
dnf -y install ohpc-base xCAT
source /etc/profile.d/xcat.sh
systemctl enable chronyd.service
echo "server 0.africa.pool.ntp.org" >> /etc/chrony.conf
echo "server 1.africa.pool.ntp.org" >> /etc/chrony.conf
echo "server 2.africa.pool.ntp.org" >> /etc/chrony.conf
echo "server 3.africa.pool.ntp.org" >> /etc/chrony.conf
echo "allow all" >> /etc/chrony.conf
systemctl restart chronyd
dnf -y install ohpc-slurm-server
chdef -t site dhcpinterfaces="xcatmn|eno1"
wget http://centos.mirror.liquidtelecom.com/8.3.2011/isos/x86_64/CentOS-8.3.2011-x86_64-dvd1.iso
copycds ./CentOS-8.3.2011-x86_64-dvd1.iso
lsdef -t osimage
export CHROOT=/install/netboot/centos8/x86_64/compute/rootimg/
genimage centos8-x86_64-netboot-compute
cp /etc/yum.repos.d/OpenHPC.repo $CHROOT/etc/yum.repos.d/
cp /etc/yum.repos.d/epel.repo $CHROOT/etc/yum.repos.d/

  ### Use this line for centOS stream
#dnf --installroot=$CHROOT --disablerepo '*' --enablerepo=extras swap centos-linux-repos centos-stream-repos

dnf -y --installroot=$CHROOT install ohpc-base-compute
dnf -y --installroot=$CHROOT update
chroot $CHROOT systemctl disable firewalld
/bin/cp -f /etc/passwd /etc/group $CHROOT/etc
dnf -y --installroot=$CHROOT install ohpc-slurm-client
echo SLURMD_OPTIONS="--conf-server 10.10.1.10" > $CHROOT/etc/sysconfig/slurmd
dnf -y --installroot=$CHROOT install chrony
echo "server 10.10.0.1" >> $CHROOT/etc/chrony.conf
dnf -y --installroot=$CHROOT install kernel-`uname -r` hwloc-libs
genimage centos8-x86_64-netboot-compute -k `uname -r`
dnf config-manager --installroot=$CHROOT --enable baseos
dnf -y --installroot=$CHROOT install --enablerepo=powertools lmod-ohpc
echo "10.10.0.1:/home /home nfs nfsvers=3,nodev,nosuid 0 0" >> $CHROOT/etc/fstab
echo "10.10.0.1:/opt/ohpc/pub /opt/ohpc/pub nfs nfsvers=3,nodev 0 0" >> $CHROOT/etc/fstab
perl -pi -e "s|/tftpboot|#/tftpboot|" /etc/exports
perl -pi -e "s|/install|#/install|" /etc/exports
echo "/home *(rw,no_subtree_check,fsid=10,no_root_squash)" >> /etc/exports
echo "/opt/ohpc/pub *(ro,no_subtree_check,fsid=11)" >> /etc/exports
exportfs -a
systemctl restart nfs-server
systemctl enable nfs-server
echo "server 10.10.0.1" >> $CHROOT/etc/chrony.conf
chroot $CHROOT systemctl enable chronyd
echo "server 10.10.0.1" >> $CHROOT/etc/chrony.conf
echo "account required pam_slurm.so" >> $CHROOT/etc/pam.d/sshd
mkdir -p /install/custom/netboot
chdef -t osimage -o centos8-x86_64-netboot-compute synclists="/install/custom/netboot/compute.synclist"
echo "/etc/passwd -> /etc/passwd" > /install/custom/netboot/compute.synclist
echo "/etc/group -> /etc/group" >> /install/custom/netboot/compute.synclist
echo "/etc/shadow -> /etc/shadow" >> /install/custom/netboot/compute.synclist
echo "/etc/slurm/slurm.conf -> /etc/slurm/slurm.conf " >> /install/custom/netboot/compute.synclist
echo "/etc/munge/munge.key -> /etc/munge/munge.key " >> /install/custom/netboot/compute.synclist
echo "/etc/hosts -> /etc/hosts " >> /install/custom/netboot/compute.synclist
chroot $CHROOT systemctl enable slurmd
chroot $CHROOT systemctl enable munge
chdef -t site domain=aau
chdef -t network net1 net=10.10.0.0 mask=255.255.0.0 gateway=10.10.0.1

mkdef -t node node1 groups=compute,all ip=10.10.1.11 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node2 groups=compute,all ip=10.10.1.12 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node3 groups=compute,all ip=10.10.1.13 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node4 groups=compute,all ip=10.10.1.14 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node5 groups=compute,all ip=10.10.1.15 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node6 groups=compute,all ip=10.10.1.16 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node7 groups=compute,all ip=10.10.1.17 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node8 groups=compute,all ip=10.10.1.18 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64
mkdef -t node node9 groups=compute,all ip=10.10.1.19 mac=<MAC_ADDRESS> netboot=xnba arch=x86_64


echo 10.10.0.10 entoto entoto.aau >> /etc/hosts
makehosts
makenetworks
makedhcp -n
makedns -n
packimage centos8-x86_64-netboot-compute
nodeset all osimage=centos8-x86_64-netboot-compute

# Configure to use default dabase for slurm in xcat(sqlite)

echo SlurmctldParameters=enable_configless >> /etc/slurm/slurm.conf

### COMPUTE NODES ARE NOW READY FOR BOOT ###

cp /etc/slurm/slurm.conf.example /etc/slurm/slurm.conf
perl -pi -e "s/ControlMachine=\S+/ControlMachine=quartz/" /etc/slurm/slurm.conf
perl -pi -e "s|ReturnToService=0|#ReturnToService=2|" /etc/slurm/slurm.conf
perl -pi -e "s|PartitionName|#PartitionName|" /etc/slurm/slurm.conf
echo PartitionName=normal Nodes=node1,node2,node3,node4, node5, node6,node7,node8,node9 Default=YES MaxTime=48:00:00 MaxNodes=2 State=UP >> \
  /etc/slurm/slurm.conf
perl -pi -e "s|NodeName|#NodeName|" /etc/slurm/slurm.conf

echo NodeName=node1 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node2 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node3 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node4 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node5 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node6 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node7 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node8 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf
echo NodeName=node9 Sockets=1 CoresPerSocket=2 ThreadsPerCore=2 State=UNKNOWN >> \
  /etc/slurm/slurm.conf

# update the states of all computing nodes
systemctl enable munge
systemctl enable slurmctld
systemctl restart munge
systemctl restart slurmctld
scontrol update nodename=node1 state=resume
scontrol update nodename=node2 state=resume
scontrol update nodename=node3 state=resume
scontrol update nodename=node4 state=resume
scontrol update nodename=node5 state=resume
scontrol update nodename=node6 state=resume
scontrol update nodename=node7 state=resume
scontrol update nodename=node8 state=resume
scontrol update nodename=node9 state=resume

dnf -y install openmpi openmpi-devel gcc-c++
dnf -y --installroot=$CHROOT install openmpi openmpi-devel gcc-c++

# Pack Diskless Image
packimage centos8-x86_64-netboot-compute

# Parallel Reboot all node(1-9)
pdsh -w node[1-9] reboot


# DHPC configuration

shared-network enp2s0f1 {
  subnet 10.10.0.0 netmask 255.255.0.0 {
    authoritative;
    max-lease-time 43200;
    min-lease-time 43200;
    default-lease-time 43200;
    option routers  10.10.0.1;
    next-server  10.10.0.1;
    option log-servers 10.10.0.1;
    option ntp-servers 10.10.0.1;
    option domain-name "entoto";
    option domain-name-servers  10.10.0.1;
    option domain-search  "entoto";
    range 10.10.1.11 10.10.1.21;
    option cumulus-provision-url "http://10.10.0.1:80/install/postscripts/cumulusztp";
    zone entoto. {
       primary 10.10.0.1; key xcat_key; 
    }
    zone 10.10.IN-ADDR.ARPA. {
       primary 10.10.0.1; key xcat_key; 
    }
    if option user-class-identifier = "xNBA" and option client-architecture = 00:00 { #x86, xCAT Network Boot Agent
        always-broadcast on;
        filename = "http://10.10.0.1:80/tftpboot/xcat/xnba/nets/10.10.0.0_16";
    } else if option user-class-identifier = "xNBA" and option client-architecture = 00:09 { #x86, xCAT Network Boot Agent
        filename = "http://10.10.0.1:80/tftpboot/xcat/xnba/nets/10.10.0.0_16.uefi";
    } else if option client-architecture = 00:00  { #x86
        filename "xcat/xnba.kpxe";
    } else if option vendor-class-identifier = "Etherboot-5.4"  { #x86
        filename "xcat/xnba.kpxe";
    } else if option client-architecture = 00:07 { #x86_64 uefi
         filename "xcat/xnba.efi";
    } else if option client-architecture = 00:09 { #x86_64 uefi alternative id
         filename "xcat/xnba.efi";
    } else if option client-architecture = 00:02 { #ia64
         filename "elilo.efi";
    } else if option client-architecture = 00:0e { #OPAL-v3
         option conf-file = "http://10.10.0.1:80/tftpboot/pxelinux.cfg/p/10.10.0.0_16";
    } else if substring (option vendor-class-identifier,0,11) = "onie_vendor" { #for onie on cumulus switch
        option www-server = "http://10.10.0.1:80/install/onie/onie-installer";
    } else if substring(filename,0,1) = null { #otherwise, provide yaboot if the client isn't specific
         filename "/yaboot";
    }
  } # 10.10.0.0/255.255.0.0 subnet_end
} # enp2s0f1 nic_end

# Routing table update
route -n 
route del -net 10.10.0.0 gw 0.0.0.0 netmask 255.255.0.0 dev enp2s0f1  # if local connection is found in the table



### GPU DRIVER AND CUDA INSTALLATION ###
dnf clean all
dnf -y update
reboot
dnf -y --enablerepo=extras install epel-release
dnf -y --enablerepo=baseos install kernel-devel-$(uname -r) kernel-headers-$(uname -r)
dnf -y install pciutils dkms
dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel8/x86_64/cuda-rhel8.repo
dnf -y module install nvidia-driver:470-dkms
reboot
dnf -y install cuda-11-2

## Loading the OpenHPC Basic Environment

#dnf -y install lmod-ohpc
#source /etc/profile.d/lmod.sh

#Modify the /root/.bashrc file.

module use /opt/ohpc/pub/modulefiles/
module use /opt/ohpc/pub/moduledeps/gnu8/
module use /opt/ohpc/pub/moduledeps/gnu8-openmpi3/

source /root/.bashrc


# swap b/n mpich openmpi

dnf module swap mpich openmpi4/4.1.1

# install fftw

dnf install -y fftw-gnu9-openmpi4-ohpc.x86_64

# add module 

module add fftw/3.3.8

# load module dependecies

module spider fftw/3.3.8




module load  gnu9/9.4.0  openmpi4/4.1.1

# load fftw/3.3.8 

 module load fftw/3.3.8

# save the modul as default

ml save

# install hdf5
dnf install -y hdf5-gnu9-ohpc

## add to env module(install version)

module add ml add hdf5/1.10.8

## save as default
ml save


# Install R

dnf install -y R-gnu9-ohpc 

## add the R to env module

 module add R/4.1.2
 
 
